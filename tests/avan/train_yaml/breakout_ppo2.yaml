alg_para:
  alg_name: PPO
  alg_config:
      prepare_times_per_train: 16

env_para:
  now_time: 2000
  env_name: EnvPool
  env_info: 
    name: BreakoutNoFrameskip-v4
    size: 5
    wait_nums: 3
    thread_affinity_offset: 0

agent_para:
  agent_name: AtariPpo2
  agent_num : 1
  agent_config:
    max_steps: 128
    complete_step: 100000



model_para:
  actor:
    model_name: PpoCnnLiteV3
    state_dim: [84, 84, 4]
    action_dim: 4
    input_dtype: uint8

    # quantization: True
    # infer_batch: 8

    model_config:
      BATCH_SIZE: 320
      CRITIC_LOSS_COEF: 1.0
      ENTROPY_LOSS: 0.003
      LOSS_CLIPPING: 0.1
      # LR: 0.00025
      LR: 0.00015
      MAX_GRAD_NORM: 5.0
      NUM_SGD_ITER: 4
      SUMMARY: False
      VF_SHARE_LAYERS: True
      activation: relu
      hidden_sizes: [256]
      # action_type: Categorical
      gpu_nums: 1
    gpu_config:
      cluster:
        peers:
      self:
        rank:


using_envpool: True
env_num: 5
speedup: True
start_core: 17

benchmark:
  log_interval_to_train: 10
  archive_root: /home/xys/xt_logs/ppo_test_variable
  id: ppo_5-3-5-4
  
# nolock: 89%
# onelock: 54%
# marylock: 50%

# env20 6073

# 3-1-3 
# 4-2-4或4-1-4
# 6-3-4

# 5-3-5 0.00025 能匹配
# 10-6-10 0.00050m 
